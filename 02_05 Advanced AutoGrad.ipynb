{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44bf6843",
   "metadata": {},
   "source": [
    "# require gradient on a variable "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f964bdd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3629dae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.1929,  0.1665,  0.6848], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fadcb070",
   "metadata": {},
   "source": [
    "This code snippet uses PyTorch to perform an iterative multiplication while tracking gradients. Here's a breakdown:\n",
    "\n",
    "1. Initializing x:\n",
    "\n",
    "x = torch.randn(3): This line creates a 1x3 tensor x filled with random numbers drawn from a standard normal distribution.\n",
    "\n",
    "x = Variable(x, requires_grad=True): (deprecated) This line (potentially from an older PyTorch version) converts x to a Variable and sets requires_grad=True. In newer versions, you can directly create the tensor with requires_grad=True:\n",
    "\n",
    "Python\n",
    "x = torch.randn(3, requires_grad=True)\n",
    "Use code with caution.\n",
    "This enables gradient tracking for x, allowing you to calculate gradients later.\n",
    "\n",
    "2. Printing Initial x:\n",
    "\n",
    "print(x): This line prints the initial values of the elements in x. The output will be three random floating-point numbers.\n",
    "3. Looping and Multiplication:\n",
    "\n",
    "y = x * 2: This line creates a new tensor y by multiplying each element of x by 2.\n",
    "\n",
    "while y.data.norm() < 1000 : This loop continues as long as the norm (magnitude) of the elements in y (represented by y.data.norm()) is less than 1000.\n",
    "\n",
    "y = y * 2: Inside the loop, y is multiplied by 2 again, effectively doubling its elements in each iteration.\n",
    "4. Potential Issues:\n",
    "\n",
    "Infinite Loop: Depending on the initial values in x, the loop might run indefinitely if the norm of y never reaches or exceeds 1000. It's generally recommended to have a maximum number of iterations to prevent this.\n",
    "\n",
    "Large Gradients: Since x has requires_grad=True, the repeated multiplications by 2 will lead to very large gradients for x after many iterations. This might cause numerical instability during training in a neural network setting.\n",
    "\n",
    "Overall, this code snippet demonstrates:\n",
    "\n",
    "Creating tensors with random values.\n",
    "Performing element-wise multiplication.\n",
    "Using a loop for iterative calculations.\n",
    "Enabling gradient tracking (though the specific use of the gradients isn't shown here)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfe326d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1035.9761, -1499.6122,    12.6534], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349b636",
   "metadata": {},
   "source": [
    "The code y.backward(gradients) performs a backward pass in PyTorch, but with a twist due to the custom gradients you provided. Here's a breakdown:\n",
    "\n",
    "Backward Pass in PyTorch:\n",
    "\n",
    "In neural networks, the backward pass calculates the gradients (rates of change) of a loss function with respect to the network's parameters (weights and biases).\n",
    "These gradients are crucial for training the network by guiding how to adjust the parameters to minimize the loss and improve performance.\n",
    "y.backward(gradients):\n",
    "\n",
    "y.backward(): This line typically initiates the backward pass, starting from the tensor y and traversing the computational graph backward to calculate gradients for all tensors with requires_grad=True involved in creating y.\n",
    "However, in this case, you're also providing a custom gradient vector gradients:\n",
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001]): This line creates a 1x3 tensor with specific values (0.1, 1.0, and 0.0001) representing the gradients you want to use instead of the ones calculated by the computational graph.\n",
    "Impact of Custom Gradients:\n",
    "\n",
    "PyTorch will disregard the gradients automatically computed during the backward pass and instead use the values in gradients for the backward propagation.\n",
    "These custom gradients will be used to update the parameters (tensors with requires_grad=True) that contributed to the calculation of y.\n",
    "Potential Use Cases:\n",
    "\n",
    "Debugging: You might use custom gradients to isolate the effect of specific parts of the network by setting gradients to zero for certain parameters.\n",
    "Guided Training: In some cases, you might have prior knowledge about the desired direction of parameter updates and could use custom gradients to influence the training process.\n",
    "Important Considerations:\n",
    "\n",
    "Using custom gradients can significantly alter the training behavior of your network. It's essential to understand the implications and use them cautiously.\n",
    "In most neural network training scenarios, it's generally recommended to rely on the automatically calculated gradients for optimal learning.\n",
    "In summary:\n",
    "\n",
    "This code snippet utilizes custom gradients instead of the ones computed during the backward pass. The specific reason for doing this depends on your intended use case. Be mindful of the potential consequences of using custom gradients, as they can significantly impact how your network learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88573ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "382ec8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.0480e+02, 2.0480e+03, 2.0480e-01])\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f71adb7",
   "metadata": {},
   "source": [
    "# Fronzen parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79ba95b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93188eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code aims to disable certificate verification for HTTPS connections in Python.\n",
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "28493221",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gupta\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\gupta\\anaconda3\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\gupta/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 44.7M/44.7M [00:01<00:00, 37.9MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fddc869",
   "metadata": {},
   "source": [
    "The code snippet for param in model.parameters(): param.requires_grad = False iterates through all the parameters (weights and biases) in a PyTorch model and sets the requires_grad attribute to False for each parameter. Here's a breakdown of what this code does:\n",
    "\n",
    "Parameters in PyTorch Models:\n",
    "\n",
    "PyTorch models typically consist of layers (e.g., linear layers, convolutional layers) that contain trainable parameters (weights and biases).\n",
    "These parameters are essential for the model's ability to learn and improve its performance on a task.\n",
    "requires_grad Attribute:\n",
    "\n",
    "The requires_grad attribute of a tensor in PyTorch determines whether gradients are calculated for that tensor during the backward pass.\n",
    "By default, parameters in a model have requires_grad=True, meaning their gradients are tracked during training.\n",
    "Effect of the Code:\n",
    "\n",
    "The code iterates through all the parameters (model.parameters()) of the model using a for loop.\n",
    "Inside the loop, for each parameter (param), it sets its requires_grad attribute to False.\n",
    "This essentially disables gradient tracking for all the parameters in the model.\n",
    "Why You Might Use This:\n",
    "\n",
    "There are a few reasons why you might want to disable gradient tracking:\n",
    "\n",
    "Freezing Layers: In transfer learning, you might want to freeze the weights of pre-trained layers in a model and only train the final layers on your specific task. Disabling gradients for the pre-trained layers prevents them from being updated during training.\n",
    "Speeding Up Training: Disabling gradients for parameters that you don't want to train can slightly improve training speed, as calculating gradients for unused parameters adds to the computational cost. However, the speedup might be negligible in many cases.\n",
    "Memory Optimization: Disabling gradients can reduce memory usage since PyTorch doesn't need to store the gradients for these parameters.\n",
    "Important Considerations:\n",
    "\n",
    "Disabling gradients for all parameters effectively prevents the model from learning and adapting. Use this technique cautiously and only for specific parts of the model that you don't intend to train.\n",
    "If you later want to train the parameters again, you'll need to explicitly set requires_grad=True for them.\n",
    "In essence:\n",
    "\n",
    "This code snippet disables gradient tracking for all parameters in a PyTorch model. This can be useful for freezing layers in transfer learning or for slight speedup and memory optimization, but use it thoughtfully considering its impact on training behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "07aa7ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all the parameters in the network\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8126e910",
   "metadata": {},
   "source": [
    "The code model.fc = nn.Linear(512, 10) in PyTorch modifies the final layer of a pre-trained ResNet-18 model (assuming that's what model refers to based on the previous line). Here's a breakdown:\n",
    "\n",
    "Components:\n",
    "\n",
    "model: This likely refers to the pre-trained ResNet-18 model you created using torchvision.models.resnet18(pretrained=True).\n",
    "nn.Linear: This class from PyTorch's nn (neural network) module is used to create fully-connected layers.\n",
    "Replacing the Final Layer:\n",
    "\n",
    "model.fc = nn.Linear(512, 10): This line assigns a new fully-connected layer to the attribute fc of the model object.\n",
    "This essentially replaces the final layer (typically used for classification) in the pre-trained ResNet-18 model with a new layer.\n",
    "512: This specifies the input size of the layer, which should match the output size of the previous layer in the ResNet-18 architecture (likely 512 based on common ResNet-18 configurations).\n",
    "10: This specifies the output size of the layer, indicating that this new layer will have 10 output features. The interpretation of these features depends on your specific task.\n",
    "Common Use Cases:\n",
    "\n",
    "There are two main reasons why you might replace the final layer of a pre-trained model:\n",
    "\n",
    "Fine-tuning for a New Classification Task: If your task has 10 different classes, this new layer with 10 outputs can be used to learn a new classification head specific to your problem. You would then train this model (including the newly added layer) on your labeled data.\n",
    "Feature Extraction: In some cases, you might only be interested in the features extracted from the pre-trained layers of the ResNet-18 model. The final classification layer (replaced here) might not be relevant. You can then use these features as input to another model or machine learning algorithm for your specific task.\n",
    "Understanding the Impact:\n",
    "\n",
    "Replacing the final layer removes the pre-trained knowledge specific to the original classification task for which the ResNet-18 was trained.\n",
    "The new layer with 10 outputs acts as a blank slate to be learned during training (if fine-tuning) or simply provides a specific feature representation (if feature extraction).\n",
    "In essence:\n",
    "\n",
    "This code snippet replaces the final layer of a pre-trained ResNet-18 model with a new fully-connected layer with 10 outputs. This modification is often used for fine-tuning the model for a new classification task or for extracting features for other applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b75a0710",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = nn.Linear(512, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c42d01",
   "metadata": {},
   "source": [
    "SGD Optimizer:\n",
    "\n",
    "#### optim.SGD: This refers to the Stochastic Gradient Descent (SGD) optimizer class from PyTorch's optim module for optimizing the model's parameters.\n",
    "#### lr=1e-2: This sets the learning rate to 0.01, which controls the step size taken during parameter updates.\n",
    "#### momentum=0.9: This sets the momentum parameter, which can help the optimizer converge faster and escape local minima.\n",
    "Overall, this code snippet creates an optimizer that specifically updates the parameters of the newly added classifier layer in your pre-trained ResNet-18 model during fine-tuning. This is a common approach to leverage pre-trained models while adapting them to new tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88b698c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize only the classifier\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7696ea68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ae54aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
